# -*- coding: utf-8 -*-
"""Handwritten Digit Recognition using MNIST and Keras/MNIST Classification with an Artificial Neural Network (ANN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ku_FPHtszu-JgXX2sBAVjjd_7grSBXPR

Simple Artificial Neural Network (ANN) using the Keras library to classify handwritten digits from the MNIST dataset.
"""

# Imports necessary libraries and modules.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist

# 1. Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Loads the MNIST dataset, splitting it into training and testing sets for images (x) and labels (y).

# 2. Preprocess data (normalize pixel values between 0 and 1)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
# Converts pixel values from integers (0-255) to floating points and normalizes them by dividing by 255.

# 3. Flatten images (reshape from 28x28 to a single array of 784 pixels)
x_train = x_train.reshape((x_train.shape[0], 784))
x_test = x_test.reshape((x_test.shape[0], 784))
# Reshapes the 28x28 images into a 1D array of 784 pixels to be used as input for the dense layers.

# 4. One-hot encode labels (convert integer labels to categorical vectors)
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
# Converts integer labels (0-9) into a binary class matrix (one-hot encoding).

# 5. Define the ANN architecture
model = Sequential()
# Initializes a sequential model, where layers are added linearly.

model.add(Dense(50, activation='relu', input_shape=(784,))) # Hidden layer 1 with 50 neurons and ReLU activation
# Adds the first dense (fully connected) hidden layer with 50 neurons, ReLU activation, and specifies the input shape.

model.add(Dense(30, activation='relu')) # Hidden layer 2 with 30 neurons and ReLU activation
# Adds the second dense hidden layer with 30 neurons and ReLU activation.

model.add(Dense(10, activation='softmax')) # Output layer with 10 neurons (one for each digit) and Softmax activation
# Adds the output layer with 10 neurons (for 10 classes) and Softmax activation to get probability distribution.

# 6. Compile the model (specify optimizer, loss function, and metrics)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# Configures the model for training with the Adam optimizer, categorical cross-entropy loss function, and accuracy metric.

# 7. Train the model
model.fit(x_train, y_train, epochs=5) # Train for 5 epochs (can be adjusted for better performance)
# Trains the model on the training data for 5 epochs.

# 8. Evaluate the model on test data
loss, accuracy = model.evaluate(x_test, y_test)
# Evaluates the trained model on the test dataset to determine its performance.

print("Test accuracy:", accuracy)
# Prints the final test accuracy.